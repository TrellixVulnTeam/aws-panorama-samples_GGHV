# Exporting Model Using NGC

The following works are done on g4dn.2xlarge + ami-0184e674549ab8432
- g4dn.xlarge should be also workable.
- ami-0184e674549ab8432 ï¼ˆDeep Learning AMI (Ubuntu 18.04) Version 60.4) which has cuda driver installed by default.

We will use TF2.5 and TF2.7 as an example to guide the user through exporting the ssd_mobilenet inside the NGC.

## Why Using NGC ?
- By using NGC, we can spare the effort of installing the cuda cudnn and tensorrt on the host machine ourselves.
- Tensorflow version 2.5 can be compiled with either TRT7 or TRT8. And NGC provides different containers that has prebuilt tensorflow 2.x + trt 7/8 inside the container.
- Some of the version is even compaitable with the settings with Jetson released tensorflow wheel file.
- For more information, please refer to
  - [Installing Tensorflow on Jetson](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)
  - [TensorFlow compatibility with NVIDIA containers and Jetpack](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform-release-notes/tf-jetson-rel.html#tf-jetson-rel)
  - [NGC Tensorflow List](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_22-06.html#rel_22-06)

With the help of NGC, we just need to choose the right Nvidia Jetson provided TF version (and wheel file), and export model with the corresponding NGC. For example:
- Nvidia provides a [Jetson TF wheel file here](https://developer.download.nvidia.com/compute/redist/jp/v461/tensorflow/tensorflow-2.7.0+nv22.1-cp36-cp36m-linux_aarch64.whl). 
- After we look up inside the table in [TensorFlow compatibility with NVIDIA containers and Jetpack](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform-release-notes/tf-jetson-rel.html#tf-jetson-rel), one can realize that this is built with Jetpack4.6.1 (i.e. TF2.7 + TRT8.2). And NGC Tensorflow 22.01 also provides the similar environment (i.e. tensorflow 2.7 + TRT8.2)
- Then we can export our model under nvcr.io/nvidia/tensorflow:22.01-tf2-py3

## Using TF2.7

### Exporting Model with TF2.7 + TRT8.2

We will export the the model inside the NGC 22.01. Which has TF2.7 + TRT8.2 installed inside the contaienr.

```
docker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS="$(id -u):$(id -g)" \
    nvcr.io/nvidia/tensorflow:22.01-tf2-py3 bash
```

Inside docker container
```
apt-get update
apt-get install protobuf-compiler
git clone https://github.com/tensorflow/models.git tensorflow_models
cd tensorflow_models/research
protoc object_detection/protos/*.proto --python_out=.
cp object_detection/packages/tf2/setup.py .

# manual edit setup.py
# - add tensorflow==2.7.0
# - change tf-models-official>=2.7.0
# - change tensorflow_io==0.23.0
python3 -m pip install .

wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz
tar zxf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz
```

To improve the model performance we are going to enable combined NMS in the post-processing

Edit ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config and add change_coordinate_frame: false and use_combined_nms: true to post_processing -> batch_non_max_suppression block.

It should look the following:
```
post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
        change_coordinate_frame: false
        use_combined_nms: true
      }
      score_converter: SIGMOID
    }
```
In order to get exported model with dynamic batch size we will use input_type float_image_tensor

Export inference graph
```
python3 object_detection/exporter_main_v2.py \
    --input_type=float_image_tensor \
    --pipeline_config_path=ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \
    --trained_checkpoint_dir=ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \
    --output_directory=output/ssd_mobilenet_v2_320x320_coco17_tpu-8_float_batchN_nms
```


### Test The Export Model and Convert to TF-TRT

Test the exported model and calculate the FPS
Let's cd to the model directory first.

`cd output/ssd_mobilenet_v2_320x320_coco17_tpu-8_float_batchN_nms`

And execute the following python3 script.

```
import tensorflow as tf
import time
m = tf.saved_model.load("saved_model")
ff = m.signatures['serving_default']
x = tf.ones(shape=(8,300,300,3))
y = ff(x)
N = 1000
t1 = time.time()
for i in range(N):
    out = ff(x)
tt = time.time() - t1
print("exec time:", tt)
print(8*N/tt, "fps")
```

Lets convert the model using the following python3 script. This will export two model:
- TF-TRT with FP16
- TF-TRT with FP32

```
import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import time
# FP16
conversion_params = trt.TrtConversionParams(precision_mode=trt.TrtPrecisionMode.FP16)
converter = trt.TrtGraphConverterV2(
    input_saved_model_dir="saved_model",
    conversion_params=conversion_params)
converter.convert()
converter.save("saved_model_trt_fp16")# FP32
conversion_params = trt.TrtConversionParams()
converter = trt.TrtGraphConverterV2(
    input_saved_model_dir="saved_model",
    conversion_params=conversion_params)
converter.convert()
converter.save("saved_model_trt_fp32")
```

After the conversion, lets try the TF-TRT, to see if the FPS is better

```
import tensorflow as tf
import time
m = tf.saved_model.load("saved_model_trt_fp16")
ff = m.signatures['serving_default']
x = tf.ones(shape=(8,300,300,3))
y = ff(x)
N = 1000
t1 = time.time()
for i in range(N):
  out = ff(x)
tt = time.time() - t1
print("exec time:", tt)
print(8*N/tt, "fps")
```

Now we can copy the folder saved_model_trt_fp16 and put it inside our Panorama app.
- We have provided the model for you already. Please execute the download_model.sh under the src folder.

## Using TF2.5 (We still recommends using TF2.7 on JP4.6.1)

The steps is quite similar to the TF2.5, just some subtle changes on setting up the environment.

```
docker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS="$(id -u):$(id -g)" \
    nvcr.io/nvidia/tensorflow:21.08-tf2-py3 bash
```

Inside docker
```
apt-get update
apt-get install protobuf-compiler
git clone https://github.com/tensorflow/models.git tensorflow_models
cd tensorflow_models/research
protoc object_detection/protos/*.proto --python_out=.
cp object_detection/packages/tf2/setup.py .
# - add tensorflow==2.5.0
# - change tf-models-official>=2.4.0
# - change tensorflow_io==0.19.0
# - change keras==2.3.1
python3 -m pip install .
wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz
tar zxf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz
```

To improve the model performance we are going to enable combined NMS in the post-processing

Edit ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config and add change_coordinate_frame: false and use_combined_nms: true to post_processing -> batch_non_max_suppression block.

It should look the following:
```
post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
        change_coordinate_frame: false
        use_combined_nms: true
      }
      score_converter: SIGMOID
    }
```
In order to get exported model with dynamic batch size we will use input_type float_image_tensor

Export inference graph
```
python3 object_detection/exporter_main_v2.py \
    --input_type=float_image_tensor \
    --pipeline_config_path=ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \
    --trained_checkpoint_dir=ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \
    --output_directory=output/ssd_mobilenet_v2_320x320_coco17_tpu-8_float_batchN_nms
```
And the rest (converting model to tf-trt) is the same as TF2.7

> Note: in tf25, you should see logs like Successfully opened dynamic library libnvinfer.so.8
> But for tf27, there will be no log anymore.

> Note: in tf27, the converted model has lower FPS in T4 GPU. While TF25 has better performance. Not sure about the reason. Need more investigations.