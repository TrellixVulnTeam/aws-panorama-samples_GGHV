{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import io\n",
    "import random\n",
    "import datetime\n",
    "import threading\n",
    "import signal\n",
    "import traceback\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Don't allocate huge memory unnecessarily\n",
    "    tf.config.experimental.set_memory_growth( gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "\n",
    "node = panoramasdk.node()\n",
    "\n",
    "def getMediasFromCamera():\n",
    "    while True:\n",
    "        media_list = node.inputs.video_in.get()\n",
    "        for media_obj in media_list:\n",
    "            if not media_obj.is_cached:\n",
    "                return media_list\n",
    "        time.sleep(0.01)\n",
    "        continue\n",
    "\n",
    "def putMediasToHdmi(media_list):\n",
    "    node.outputs.video_out.put(media_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list = getMediasFromCamera()\n",
    "\n",
    "media_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list[0].image.shape, media_list[0].image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewImage( image ):\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure( figsize = ( 10, 10 ) )\n",
    "    plt.imshow( image_rgb, interpolation='antialiased' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previewImage(media_list[0].image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "putMediasToHdmi(media_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop():\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            media_list = getMediasFromCamera()\n",
    "\n",
    "            putMediasToHdmi( media_list[:1] )\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainLoop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual steps:**\n",
    "\n",
    "1. Download ssd_mobilenet_v2_2.tar.gz from https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2.\n",
    "1. Upload the file to next to this notebook.\n",
    "1. Create a directory \"./ssd_mobilenet_v2_2\"\n",
    "1. Extract the contents of the model file under \"./ssd_mobilenet_v2_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_2\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = model.signatures[\"serving_default\"]\n",
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndDetect( image ):\n",
    "\n",
    "    input_resolution = ( 320, 320 )\n",
    "    \n",
    "    image = tf.expand_dims( image, axis=0 )\n",
    "\n",
    "    image = tf.image.resize( image, input_resolution )\n",
    "\n",
    "    # BGR to RGB\n",
    "    image = tf.reverse(image, axis=[-1])\n",
    "\n",
    "    image = tf.cast( image, dtype=tf.uint8 )\n",
    "\n",
    "    result = detector(image)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_result = preprocessAndDetect( media_list[0].image )\n",
    "\n",
    "detection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.5\n",
    "box_color = (255,0,0)\n",
    "box_thickness = 2\n",
    "\n",
    "def renderResult( image, detection_result ):\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    detection_classes = detection_result[\"detection_classes\"][0].numpy()\n",
    "    detection_scores = detection_result[\"detection_scores\"][0].numpy()\n",
    "    detection_boxes = detection_result[\"detection_boxes\"][0].numpy()\n",
    "\n",
    "    for klass, score, box in zip( detection_classes, detection_scores, detection_boxes ):\n",
    "        if klass == 1: # person\n",
    "            if score >= score_threshold:\n",
    "    \n",
    "                box_in_camera_space = (\n",
    "                    int( box[1].item() * w ),\n",
    "                    int( box[0].item() * h ),\n",
    "                    int( box[3].item() * w ),\n",
    "                    int( box[2].item() * h ), \n",
    "                )\n",
    "\n",
    "                cv2.rectangle( \n",
    "                    image, \n",
    "                    box_in_camera_space[0:2], \n",
    "                    box_in_camera_space[2:4], \n",
    "                    color = box_color, thickness = box_thickness, lineType=cv2.LINE_8\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderResult( media_list[0].image, detection_result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previewImage(media_list[0].image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_positions_x = []\n",
    "people_positions_y = []\n",
    "people_positions_timestamp = []\n",
    "\n",
    "def trackPeoplePositions( detection_result ):\n",
    "\n",
    "    global people_positions_x, people_positions_y, people_positions_timestamp\n",
    "\n",
    "    t_now = time.time()\n",
    "\n",
    "    #num_detections = float( result[\"num_detections\"][0] )\n",
    "    detection_classes = detection_result[\"detection_classes\"][0].numpy()\n",
    "    detection_scores = detection_result[\"detection_scores\"][0].numpy()\n",
    "    detection_boxes = detection_result[\"detection_boxes\"][0].numpy()\n",
    "\n",
    "    # add detected positions (bottom-center of boxes)\n",
    "    for klass, score, box in zip( detection_classes, detection_scores, detection_boxes ):\n",
    "        if klass == 1: # person\n",
    "            if score >= 0.5:\n",
    "                people_positions_x.append( ( box[1] + box[3] ) * 0.5 )\n",
    "                people_positions_y.append( box[2] )\n",
    "                people_positions_timestamp.append( t_now )\n",
    "\n",
    "    # forget old positions\n",
    "    max_duration = 5 * 60 # 5min\n",
    "    #max_duration = 60 * 60 # 1hour\n",
    "    for i, t in enumerate( people_positions_timestamp ):\n",
    "        if t > t_now-max_duration:\n",
    "            break\n",
    "\n",
    "    people_positions_x = people_positions_x[i:]\n",
    "    people_positions_y = people_positions_y[i:]\n",
    "    people_positions_timestamp = people_positions_timestamp[i:]\n",
    "\n",
    "    #print( \"Number of data points :\", len(people_positions_timestamp) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackPeoplePositions( detection_result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_resolution = (90,160)\n",
    "heatmap_sigma = 5\n",
    "\n",
    "def renderHeatmap():\n",
    "\n",
    "    fig, ax1 = plt.subplots( nrows = 1, ncols = 1, figsize=( 16, 9 ) )\n",
    "\n",
    "    img, xedges, yedges = np.histogram2d( people_positions_y, people_positions_x, bins=heatmap_resolution, range=((0,1),(0,1)) )\n",
    "    \n",
    "    img = cv2.GaussianBlur( img, (0,0), heatmap_sigma, cv2.BORDER_DEFAULT )\n",
    "\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.imshow(img, cmap=matplotlib.cm.jet)\n",
    "\n",
    "    fig.tight_layout( pad=0 )\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer( fig.canvas.tostring_rgb(), dtype=np.uint8 )\n",
    "    fig_w, fig_h = fig.canvas.get_width_height()\n",
    "    img = img.reshape( ( fig_h, fig_w, 3 ) )\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = renderHeatmap()\n",
    "\n",
    "heatmap.shape, heatmap.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "previewImage( heatmap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlayHeatmap( dst_image, heatmap, weight=0.5 ):\n",
    "    resized_heatmap = cv2.resize( heatmap, ( dst_image.shape[1], dst_image.shape[0] ))\n",
    "    blended = cv2.addWeighted( dst_image, 1-weight, resized_heatmap, weight, 0.0 )\n",
    "    dst_image[:,:,:] = blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayHeatmap( media_list[0].image, heatmap, 0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previewImage( media_list[0].image )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_color = (255,255,255)\n",
    "text_shadow_color = (0,0,0)\n",
    "text_thickness = 2\n",
    "text_shadow_thickness = 2\n",
    "text_scale = 2\n",
    "\n",
    "def renderTitle( image, s ):\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    cv2.putText( image, s, (22, 40+2), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=text_scale, color=text_shadow_color, thickness=text_shadow_thickness, lineType=cv2.LINE_AA )\n",
    "    cv2.putText( image, s, (20, 40), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=text_scale, color=text_color, thickness=text_thickness, lineType=cv2.LINE_AA )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop():\n",
    "    try:\n",
    "        heatmap = None\n",
    "        t_heatmap = 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            media_list = getMediasFromCamera()\n",
    "            \n",
    "            if media_list[0].is_cached:\n",
    "                time.sleep(0.03)\n",
    "                continue\n",
    "\n",
    "            detection_result = preprocessAndDetect( media_list[0].image )\n",
    "            trackPeoplePositions(detection_result)\n",
    "            \n",
    "            if heatmap is None or time.time() - t_heatmap > 5:\n",
    "                heatmap = renderHeatmap()\n",
    "                t_heatmap = time.time()\n",
    "            \n",
    "            overlayHeatmap( media_list[0].image, heatmap )\n",
    "            \n",
    "            renderResult( media_list[0].image, detection_result )\n",
    "            \n",
    "            renderTitle( media_list[0].image, \"Retail - traffic analysis by heatmap\" )\n",
    "            \n",
    "            putMediasToHdmi( media_list[:1] )\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cProfile.runctx( \"mainLoop()\", globals(), locals() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
