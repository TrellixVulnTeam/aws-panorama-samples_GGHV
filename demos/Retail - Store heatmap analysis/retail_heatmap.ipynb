{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    # Don't allocate huge memory unnecessarily\n",
    "    tf.config.experimental.set_memory_growth( gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define panoramasdk wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panoramasdk\n",
    "\n",
    "node = panoramasdk.node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_media_list = []\n",
    "\n",
    "class CopiedMedia:\n",
    "    pass\n",
    "\n",
    "def getMediasFromCamera():\n",
    "    \n",
    "    copied_media_list = []\n",
    "\n",
    "    media_list = node.inputs.video_in.get()\n",
    "    \n",
    "    latest_media_list[:] = media_list\n",
    "    \n",
    "    for media_obj in media_list:\n",
    "        copied_media = CopiedMedia()\n",
    "        copied_media.image = media_obj.image.copy()\n",
    "        copied_media.is_cached = media_obj.is_cached\n",
    "        copied_media.stream_uri = media_obj.stream_uri\n",
    "        copied_media.stream_id = media_obj.stream_id\n",
    "        copied_media.time_stamp = media_obj.time_stamp\n",
    "        copied_media_list.append(copied_media)\n",
    "    \n",
    "    return copied_media_list\n",
    "\n",
    "def putMediasToHdmi(copied_media_list):\n",
    "    \n",
    "    for media_obj, copied_media in zip( latest_media_list, copied_media_list ):\n",
    "        media_obj.image[:] = copied_media.image\n",
    "    \n",
    "    node.outputs.video_out.put(latest_media_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm input from Camera and ouput to HDMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list = getMediasFromCamera()\n",
    "\n",
    "media_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list[0].image.shape, media_list[0].image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previewImage( image ):\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure( figsize = ( 10, 10 ) )\n",
    "    plt.imshow( image_rgb, interpolation='antialiased' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previewImage(media_list[0].image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "putMediasToHdmi(media_list[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists( \"./ssd_mobilenet_v2_320x320_coco17_tpu-8\" ):\n",
    "    !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
    "    !tar xvzf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = model.signatures[\"serving_default\"]\n",
    "detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resolution = ( 300, 300 )\n",
    "    \n",
    "def preprocessAndDetect( image_list ):\n",
    "    \n",
    "    np_images = np.array( image_list )\n",
    "    \n",
    "    tf_input = tf.convert_to_tensor( np_images )\n",
    "\n",
    "    tf_input = tf.image.resize( tf_input, input_resolution )\n",
    "\n",
    "    # BGR to RGB\n",
    "    tf_input = tf.reverse(tf_input, axis=[-1])\n",
    "\n",
    "    tf_input = tf.cast( tf_input, dtype=tf.uint8 )\n",
    "\n",
    "    result = detector(tf_input)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_result = preprocessAndDetect( [ media_list[0].image ] )\n",
    "\n",
    "detection_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.5\n",
    "box_color = (255,0,0)\n",
    "box_thickness = 2\n",
    "\n",
    "def renderResult( image, detection_result ):\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    detection_classes = detection_result[\"detection_classes\"][0].numpy()\n",
    "    detection_scores = detection_result[\"detection_scores\"][0].numpy()\n",
    "    detection_boxes = detection_result[\"detection_boxes\"][0].numpy()\n",
    "\n",
    "    for klass, score, box in zip( detection_classes, detection_scores, detection_boxes ):\n",
    "        if klass == 1: # person\n",
    "            if score >= score_threshold:\n",
    "    \n",
    "                box_in_camera_space = (\n",
    "                    int( box[1].item() * w ),\n",
    "                    int( box[0].item() * h ),\n",
    "                    int( box[3].item() * w ),\n",
    "                    int( box[2].item() * h ), \n",
    "                )\n",
    "\n",
    "                cv2.rectangle( \n",
    "                    image, \n",
    "                    box_in_camera_space[0:2], \n",
    "                    box_in_camera_space[2:4], \n",
    "                    color = box_color, thickness = box_thickness, lineType=cv2.LINE_8\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderResult( media_list[0].image, detection_result )\n",
    "\n",
    "previewImage(media_list[0].image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_positions_x = []\n",
    "people_positions_y = []\n",
    "people_positions_timestamp = []\n",
    "\n",
    "forget_after = 30 # 30 sec\n",
    "#forget_after = 5 * 60 # 5min\n",
    "#forget_after = 60 * 60 # 1hour\n",
    "\n",
    "def trackPeoplePositions( detection_result ):\n",
    "\n",
    "    global people_positions_x, people_positions_y, people_positions_timestamp\n",
    "\n",
    "    t_now = time.time()\n",
    "\n",
    "    #num_detections = float( result[\"num_detections\"][0] )\n",
    "    detection_classes = detection_result[\"detection_classes\"][0].numpy()\n",
    "    detection_scores = detection_result[\"detection_scores\"][0].numpy()\n",
    "    detection_boxes = detection_result[\"detection_boxes\"][0].numpy()\n",
    "\n",
    "    # add detected positions (bottom-center of boxes)\n",
    "    for klass, score, box in zip( detection_classes, detection_scores, detection_boxes ):\n",
    "        if klass == 1: # person\n",
    "            if score >= 0.5:\n",
    "                people_positions_x.append( ( box[1] + box[3] ) * 0.5 )\n",
    "                people_positions_y.append( box[2] )\n",
    "                people_positions_timestamp.append( t_now )\n",
    "\n",
    "    # forget old positions\n",
    "    for i, t in enumerate( people_positions_timestamp ):\n",
    "        if t > t_now-forget_after:\n",
    "            break\n",
    "\n",
    "    people_positions_x = people_positions_x[i:]\n",
    "    people_positions_y = people_positions_y[i:]\n",
    "    people_positions_timestamp = people_positions_timestamp[i:]\n",
    "\n",
    "    #print( \"Number of data points :\", len(people_positions_timestamp) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackPeoplePositions( detection_result )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_resolution = (90,160)\n",
    "heatmap_sigma = 5\n",
    "\n",
    "def renderHeatmap():\n",
    "\n",
    "    fig, ax1 = plt.subplots( nrows = 1, ncols = 1, figsize=( 16, 9 ) )\n",
    "\n",
    "    img, xedges, yedges = np.histogram2d( people_positions_y, people_positions_x, bins=heatmap_resolution, range=((0,1),(0,1)) )\n",
    "    \n",
    "    img = cv2.GaussianBlur( img, (0,0), heatmap_sigma, cv2.BORDER_DEFAULT )\n",
    "\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.imshow(img, cmap=matplotlib.cm.jet)\n",
    "\n",
    "    fig.tight_layout( pad=0 )\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer( fig.canvas.tostring_rgb(), dtype=np.uint8 )\n",
    "    fig_w, fig_h = fig.canvas.get_width_height()\n",
    "    img = img.reshape( ( fig_h, fig_w, 3 ) )\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = renderHeatmap()\n",
    "\n",
    "heatmap.shape, heatmap.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "previewImage( heatmap )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlayHeatmap( dst_image, heatmap, weight=0.5 ):\n",
    "    resized_heatmap = cv2.resize( heatmap, ( dst_image.shape[1], dst_image.shape[0] ))\n",
    "    blended = cv2.addWeighted( dst_image, 1-weight, resized_heatmap, weight, 0.0 )\n",
    "    dst_image[:,:,:] = blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayHeatmap( media_list[0].image, heatmap, 0.5 )\n",
    "\n",
    "previewImage( media_list[0].image )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render demo title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_color = (255,255,255)\n",
    "text_shadow_color = (0,0,0)\n",
    "text_thickness = 2\n",
    "text_shadow_thickness = 2\n",
    "text_scale = 2\n",
    "\n",
    "def renderTitle( image, s ):\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    \n",
    "    cv2.putText( image, s, (22, 40+2), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=text_scale, color=text_shadow_color, thickness=text_shadow_thickness, lineType=cv2.LINE_AA )\n",
    "    cv2.putText( image, s, (20, 40), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=text_scale, color=text_color, thickness=text_thickness, lineType=cv2.LINE_AA )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderTitle( media_list[0].image, \"Retail - traffic analysis by heatmap\" )\n",
    "\n",
    "previewImage( media_list[0].image )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mainloop (single thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainLoop():\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            media_list = getMediasFromCamera()\n",
    "            \n",
    "            detection_result = preprocessAndDetect( [ media_list[0].image ] )\n",
    "\n",
    "            trackPeoplePositions(detection_result)\n",
    "            \n",
    "            heatmap = renderHeatmap()\n",
    "            \n",
    "            overlayHeatmap( media_list[0].image, heatmap )\n",
    "            \n",
    "            renderResult( media_list[0].image, detection_result )\n",
    "            \n",
    "            renderTitle( media_list[0].image, \"Retail - traffic analysis by heatmap\" )\n",
    "            \n",
    "            putMediasToHdmi( media_list[:1] )\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mainLoop()\n",
    "#cProfile.runctx( \"mainLoop()\", globals(), locals() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi threaded mainloop for smoother video on HDMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceThread(threading.Thread):\n",
    "    \n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.is_canceled = False\n",
    "        self.queue = []\n",
    "        self.heatmap = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.batch_size = 1\n",
    "    \n",
    "    def enqueue( self, image ):\n",
    "        self.lock.acquire()\n",
    "        try:\n",
    "            if len(self.queue) < self.batch_size + 1:\n",
    "                self.queue.append(image)\n",
    "        finally:\n",
    "            self.lock.release()\n",
    "    \n",
    "    def run(self):\n",
    "        while not self.is_canceled:\n",
    "            \n",
    "            self.lock.acquire()\n",
    "            try:\n",
    "                if len(self.queue)>=self.batch_size:                    \n",
    "                    batch = self.queue[:self.batch_size]\n",
    "                    del self.queue[:self.batch_size]\n",
    "                else:\n",
    "                    time.sleep(0.1)\n",
    "                    continue                    \n",
    "            finally:\n",
    "                self.lock.release()\n",
    "                    \n",
    "            detection_result = preprocessAndDetect(batch)\n",
    "            trackPeoplePositions(detection_result)\n",
    "            self.heatmap = renderHeatmap()\n",
    "    \n",
    "    def cancel(self):\n",
    "        self.is_canceled = True\n",
    "\n",
    "def mainLoop():\n",
    "    inference_thread = InferenceThread()\n",
    "    inference_thread.start()\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            media_list = getMediasFromCamera()\n",
    "            \n",
    "            inference_thread.enqueue( media_list[0].image )\n",
    "            \n",
    "            if inference_thread.heatmap is not None:\n",
    "                overlayHeatmap( media_list[0].image, inference_thread.heatmap )\n",
    "            \n",
    "            #renderResult( media_list[0].image, detection_result )\n",
    "            \n",
    "            renderTitle( media_list[0].image, \"Retail - traffic analysis by heatmap\" )\n",
    "            \n",
    "            putMediasToHdmi( media_list[:1] )\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    finally:\n",
    "        inference_thread.cancel()\n",
    "        inference_thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
